# -*- coding: utf-8 -*-
"""lda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TioD_V-oNGmgTeUJd03UaSg5gCnREPh-
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from IPython.display import display
from tqdm import tqdm
from collections import Counter
import ast

import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import seaborn as sb


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from textblob import TextBlob
import scipy.stats as stats

from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.manifold import TSNE

from bokeh.plotting import figure, output_file, show
from bokeh.models import Label
from bokeh.io import output_notebook
output_notebook()

# %matplotlib inline

pip install pyLDAvis

import nltk
nltk.download('all')

from google.colab import files
uploaded = files.upload()

print(uploaded.keys())

"""# read file into dataframe"""

import io
df_stack = pd.read_parquet(io.BytesIO(uploaded['stack_questions (1).parquet']))

df_stack.shape #check shape

df_stack.describe() #description of the dataset

df_stack.duplicated().sum() #check for duplicated rows

df_stack.head()

df_stack['tags'].nunique() #number of unique tags in the dataset

reindexed_summary=df_stack['summary'] #assign the summary column to a variable
questions = df_stack['question'] #assign the question column to a variable
combo = df_stack['summary'] + df_stack['question'] #assign the combination of both columns to a variable

combo[8]

# Remove '\n' from each word in the dataset
combo = [word.replace('\n', '') for word in combo]
# Remove '\n' from each word in the dataset
reindexed_summary =  [word.replace('\n', '') for word in reindexed_summary]
# Remove '\n' from each word in the dataset
questions = [word.replace('\n', '') for word in questions]


#convert to series
reindexed_summary = pd.Series(reindexed_summary)
questions = pd.Series(questions)
combo = pd.Series(combo)

# Initializing a CountVectorizer object with specific parameters:
# stop words are set to 'english' to filter out common English words that don't provide much information.
# The maximum number of features (unique words) the vectorizer should consider is set to 40,000.
count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)

# Randomly sampling 9,000 observations from 'reindexed_summary' for processing.
# The 'random_state' is set to 0 to ensure reproducibility.
small_text_sample = reindexed_summary.sample(n=9000, random_state=0).values

# Printing a sample headline before vectorization for comparison.
print('Headline before vectorization: {}'.format(small_text_sample[123]))

# Fitting the CountVectorizer to the sampled text and transforming it to a document-term matrix.
# This matrix will represent the frequency of each word in each document.
small_document_term_matrix = count_vectorizer.fit_transform(small_text_sample)

# Printing the same sample headline after vectorization to show the transformed representation.
print('Headline after vectorization: \n{}'.format(small_document_term_matrix[123]))



n_topics = 100 #randomly setting the number of topics to 100

"""# Training the model using count vectorizer matrix"""

# Initializing an LDA model with specified parameters:

count_lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online',
                                      random_state=42, verbose=0)

# Fitting the LDA model to the document-term matrix created previously.

count_lda_topic_matrix = count_lda_model.fit_transform(small_document_term_matrix)

# Using the 'argmax' function to find the index of the maximum value in the first document's topic distribution.
# This index corresponds to the dominant topic for that particular document.
dominant_topic = np.argmax(count_lda_topic_matrix[0])

# Printing  the dominant topic index for the first document.
dominant_topic

tfidf_vectorizer = TfidfVectorizer()

from sklearn.feature_extraction.text import TfidfVectorizer

# Initializing a TfidfVectorizer object with specific parameters:

# The maximum number of features (unique words) the vectorizer should consider is set to 40,000.
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=40000)

# Fitting the TfidfVectorizer to the sampled text and transforming it to a TF-IDF matrix.
small_tfidf_matrix = tfidf_vectorizer.fit_transform(small_text_sample)

# Sample output
print('Sample text:', small_text_sample[100])
print('TF-IDF values for the text:\n', small_tfidf_matrix[100])

"""# Training the model using tfidf vectorizer"""

tfidf_lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online',
                                      random_state=0, verbose=0)


tfidf_lda_model.fit(small_tfidf_matrix)

tfidf_lda_topic_matrix = tfidf_lda_model.transform(small_tfidf_matrix)

def get_sklearn_lda_topics(model, feature_names, n_words=10):
    word_dict = {}
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[-n_words:][::-1]
        top_words = [feature_names[i] for i in top_features_ind]
        word_dict['Topic # ' + '{:02d}'.format(topic_idx+1)] = top_words
    return list(word_dict.values())

"""# Extract topics from both LDA models"""

# For CountVectorizer's LDA
count_feature_names = count_vectorizer.get_feature_names_out()
count_topics = get_sklearn_lda_topics(count_lda_model, count_feature_names)

# For TfidfVectorizer's LDA
tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
tfidf_topics = get_sklearn_lda_topics(tfidf_lda_model, tfidf_feature_names)

tfidf_feature_names

"""# Calculate coherence scores for both sets of topics"""

from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora.dictionary import Dictionary

texts_tokenized = [text.split() for text in small_text_sample]
dictionary = Dictionary(texts_tokenized)

# Coherence Score for CountVectorizer's LDA
coherence_model_count = CoherenceModel(topics=count_topics, texts=texts_tokenized, dictionary=dictionary, coherence='c_v')
coherence_count = coherence_model_count.get_coherence()

# Coherence Score for TfidfVectorizer's LDA
coherence_model_tfidf = CoherenceModel(topics=tfidf_topics, texts=texts_tokenized, dictionary=dictionary, coherence='c_v')
coherence_tfidf = coherence_model_tfidf.get_coherence()

print('Coherence Score (CountVectorizer): ', coherence_count)
print('Coherence Score (TfidfVectorizer): ', coherence_tfidf)

"""#training the model using gensim"""

import gensim
from gensim.corpora import Dictionary
from gensim.models.ldamodel import LdaModel

# 1. Tokenization
texts_tokenized = [text.split() for text in small_text_sample]  # Assuming small_text_sample contains your texts

# 2. Create a Gensim dictionary from the tokenized data
dictionary = Dictionary(texts_tokenized)

# 3. Convert tokenized documents to vectors
corpus = [dictionary.doc2bow(text) for text in texts_tokenized]


gensim_lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics,
                            random_state=42, update_every=1, passes=1)  # using 'online' approach with 'passes=1' and 'update_every=1'

# To get the topic distribution for a document, we use:
for idx, text in enumerate(texts_tokenized[:10]):
    bow = dictionary.doc2bow(text)
    doc_distribution = gensim_lda_model[bow]
    print(f"Document {idx} topic distribution: {doc_distribution}")

for idx, topic in gensim_lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

import pyLDAvis.gensim_models
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(gensim_lda_model, corpus, dictionary)
vis



"""# Tuning the hyperparameters"""

from gensim.models import LdaMulticore
from gensim.models.coherencemodel import CoherenceModel

# Define the parameter grid
topic_range = list(range(2, 30, 2))  # Adjust as necessary
alpha = list(np.arange(0.01, 1, 0.3)) + ['symmetric', 'asymmetric']
beta = list(np.arange(0.01, 1, 0.3)) + ['symmetric']

# Define function to compute coherence
def compute_coherence_values(corpus, dictionary, texts, num_topics, alpha, beta):
    lda_model = LdaMulticore(corpus=corpus,
                             id2word=dictionary,
                             num_topics=num_topics,
                             alpha=alpha,
                             eta=beta,
                             random_state=0)

    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')

    return coherence_model.get_coherence()

# Grid Search
model_results = {'Topics': [], 'Alpha': [], 'Beta': [], 'Coherence': []}

for k in topic_range:
    for a in alpha:
        for b in beta:
            cv = compute_coherence_values(corpus=corpus, dictionary=dictionary, texts=texts_tokenized, num_topics=k, alpha=a, beta=b)
            model_results['Topics'].append(k)
            model_results['Alpha'].append(a)
            model_results['Beta'].append(b)
            model_results['Coherence'].append(cv)
            print(f"num_topics={k}, alpha={a}, beta={b}, Coherence Score={cv}")

# Your results will be stored in `model_results`. You can convert it to a DataFrame for easier viewing and find the best parameters based on the highest coherence score:
import pandas as pd

df = pd.DataFrame(model_results)
best_params = df.loc[df['Coherence'].idxmax()]

print(best_params)

from gensim.models.coherencemodel import CoherenceModel
coherence_model_lda = CoherenceModel(model=gensim_lda_model, texts=texts_tokenized, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Define helper functions
def get_keys(topic_matrix):
    '''
    returns an integer list of predicted topic
    categories for a given topic matrix
    '''
    keys = topic_matrix.argmax(axis=1).tolist()
    return keys

def keys_to_counts(keys):
    '''
    returns a tuple of topic categories and their
    accompanying magnitudes for a given list of keys
    '''
    count_pairs = Counter(keys).items()
    categories = [pair[0] for pair in count_pairs]
    counts = [pair[1] for pair in count_pairs]
    return (categories, counts)

def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):
    '''
    returns a list of n_topic strings, where each string contains the n most common
    words in a predicted category, in order
    '''
    top_word_indices = []
    for topic in range(n_topics):
        temp_vector_sum = 0
        for i in range(len(keys)):
            if keys[i] == topic:
                temp_vector_sum += document_term_matrix[i]
        temp_vector_sum = temp_vector_sum.toarray()
        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)
        top_word_indices.append(top_n_word_indices)
    top_words = []
    for topic in top_word_indices:
        topic_words = []
        for index in topic:
            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))
            temp_word_vector[:,index] = 1
            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]
            topic_words.append(the_word.encode('ascii').decode('utf-8'))
        top_words.append(" ".join(topic_words))
    return top_words

lda_keys = get_keys(count_lda_topic_matrix)
lda_categories, lda_counts = keys_to_counts(lda_keys)

top_n_words_lda = get_top_n_words(10, lda_keys, small_document_term_matrix, count_vectorizer)

for i in range(len(top_n_words_lda)):
    print("Topic {}: ".format(i+1), top_n_words_lda[i])

top_3_words = get_top_n_words(3, lda_keys, small_document_term_matrix, count_vectorizer)
labels = ['Topic {}: \n'.format(i) + top_3_words[i] for i in lda_categories]

fig, ax = plt.subplots(figsize=(16,8))
ax.bar(lda_categories, lda_counts);
ax.set_xticks(lda_categories);
ax.set_xticklabels(labels);
ax.set_title('LDA topic counts');
ax.set_ylabel('Number of headlines');

# Importing the TSNE  for dimensionality reduction
# for the visualization of high-dimensional datasets
from sklearn.manifold import TSNE

# Initializing a TSNE model with specific parameters

tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100,
                      n_iter=2000, verbose=1, random_state=0, angle=0.75)

# Fitting the TSNE model to the lda_topic_matrix to reduce its dimensionality.
# The result 'tsne_lda_vectors' will be a 2D representation of the original high-dimensional lda_topic_matrix
tsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)

def get_mean_topic_vectors(keys, two_dim_vectors):
    '''
    returns a list of centroid vectors from each predicted topic category
    '''
    mean_topic_vectors = []
    for t in range(n_topics):
        articles_in_that_topic = []
        for i in range(len(keys)):
            if keys[i] == t:
                articles_in_that_topic.append(two_dim_vectors[i])

        articles_in_that_topic = np.vstack(articles_in_that_topic)
        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)
        mean_topic_vectors.append(mean_article_in_that_topic)
    return mean_topic_vectors

colormap = np.array([
    "#1f77b4", "#aec7e8", "#ff7f0e", "#ffbb78", "#2ca02c",
    "#98df8a", "#d62728", "#ff9896", "#9467bd", "#c5b0d5",
    "#8c564b", "#c49c94", "#e377c2", "#f7b6d2", "#7f7f7f",
    "#c7c7c7", "#bcbd22", "#dbdb8d", "#17becf", "#9edae5" ])
colormap = colormap[:n_topics]

top_3_words_lda = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)
lda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)

plot = figure(title="t-SNE Clustering of {} LDA Topics".format(n_topics), width=700, height=700)
plot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])

for t in range(n_topics):
    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1],
                  text=top_3_words_lda[t], text_color=colormap[t])
    plot.add_layout(label)

show(plot)

lda_perplexity = lda_model.perplexity(small_document_term_matrix)
f'Perplexity: {lda_perplexity}'

import gensim
from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora.dictionary import Dictionary


# Assuming your original texts are stored in 'small_text_sample'
# Tokenize your texts
texts_tokenized = [text.split() for text in small_text_sample]

# Create a Gensim dictionary from the tokenized data
dictionary = Dictionary(texts_tokenized)

# Convert tokenized documents to vectors
corpus = [dictionary.doc2bow(text) for text in texts_tokenized]

# Extract the top terms for each topic
def get_lda_topics(model, num_topics, num_words=10):
    word_dict = {}
    feature_names = tfidf_vectorizer.get_feature_names_out()
    for i in range(num_topics):
        words = model.components_[i].argsort()[-num_words:][::-1]
        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [terms[i] for i in words]
    return word_dict

top_terms = get_lda_topics(lda_model, n_topics)

# Convert the top topic terms into the format required for CoherenceModel
topics = [topic for _, topic in top_terms.items()]

# Compute Coherence Score using Gensim's CoherenceModel
coherence_model_lda = CoherenceModel(topics=topics, texts=texts_tokenized, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()

print('\nCoherence Score: ', coherence_lda)

tuning_sample = reindexed_summary.sample(frac=0.2)
len(tuning_sample)

tfidf_vectorizer.fit(tuning_sample)

tuning_sample_matrix = tfidf_vectorizer.transform(tuning_sample)

from sklearn.model_selection import GridSearchCV

# Define Search Param
search_params = {
    'n_components': [10, 15, 20, 25, 30],
    'learning_decay': [.5, .7, .9],
    'max_iter': [10, 20, 30]
}

# Initialize the model
lda = LatentDirichletAllocation()

# Use GridSearchCV
model = GridSearchCV(lda, param_grid=search_params)

# Fit Grid Search
model.fit(tuning_sample_matrix)

# Best Model
best_lda_model = model.best_estimator_

print("Best Model's Params: ", model.best_params_)
print("Best Log Likelihood Score: ", model.best_score_)
print("Model Perplexity: ", best_lda_model.perplexity(tuning_sample_matrix))



t

import pyLDAvis
import pyLDAvis.sklearn

pyLDAvis.enable_notebook()

# Assuming lda_model is your trained LatentDirichletAllocation model and
# small_document_term_matrix is the document-term matrix you used for training
panel = pyLDAvis.sklearn.prepare(lda_model, small_document_term_matrix, small_count_vectorizer, mds='tsne')
panel



"""TOP2VEC"""

df.head()

docs = df.summary.tolist()

docs[0]

from top2vec import Top2Vec

model = Top2Vec(docs)

topic_sizes, topic_nums = model.get_topic_sizes()
print(topic_sizes)

print(topic_nums)

topic_words, word_scores, topic_nums = model.get_topics(3)

for words, scores, num in zip(topic_words, word_scores, topic_nums):
  print(num)
  print(f'words: {words}')



"""# count vectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

# Initialize the Count Vectorizer
count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)

# Fit the Count Vectorizer on your text data
dtm = count_vectorizer.fit(small_text_sample)

# Retrieve the feature names (terms) from the count_vectorizer.
# This returns the list of words corresponding to each column in the document-term matrix.
terms = count_vectorizer.get_feature_names_out()

# Assuming you've already trained an LDA model called lda_model...
# (If not, ensure you train the LDA model on the Count Vectorized data.)

# Setting the topic ID for which you want to find the top words.
topic_id = 0

# For the specified topic, get the indices of the top 10 words.
top_words_indices = lda_model.components_[topic_id].argsort()[-10:][::-1]

# Use list comprehension to retrieve the actual words/terms corresponding to the top indices.
top_words = [terms[i] for i in top_words_indices]

# Print or return the top words for the specified topic.
print(top_words)

n_topics = 10  # Adjust based on your requirement
lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda_output = lda_model.fit_transform(dtm)

# Perplexity Score
perplexity = lda_model.perplexity(dtm)
print(f"Perplexity Score: {perplexity}")

# Coherence Score (Using Gensim)
# Tokenize your data for Gensim
tokenized_data = [text.split() for text in small_text_sample]

# Create a Gensim dictionary and corpus
gensim_dictionary = Dictionary(tokenized_data)
gensim_corpus = [gensim_dictionary.doc2bow(doc) for doc in tokenized_data]

# Extract topics from LDA model
topics = []
for i, topic in enumerate(lda_model.components_):
    topics.append([gensim_dictionary.id2token[id] for id in topic.argsort()[-10:]])

# Compute Coherence Score using Gensim's CoherenceModel
coherence_model = CoherenceModel(topics=topics, texts=tokenized_data, dictionary=gensim_dictionary, coherence='c_v')
coherence = coherence_model.get_coherence()
print(f"Coherence Score: {coherence}")

"""# Training the Best model"""

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer



# Vectorization: Convert documents to a matrix of token counts
vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')
tuning_sample_matrix = vectorizer.fit_transform(combo)

# Setting up the LDA model with provided hyperparameters
best_lda_model = LatentDirichletAllocation(
    n_components=28,               # Number of topics
    doc_topic_prior=0.31,          # Alpha
    topic_word_prior=0.31,         # Beta
    learning_method='online',      # Use 'online' or 'batch' learning
    random_state=0,                # Seed for reproducibility
    n_jobs=-1                      # Use all available CPUs
)

# Fitting the LDA model
best_lda_model.fit(tuning_sample_matrix)

def display_topics(model, feature_names, no_top_words):
    topics = []
    for topic_idx, topic in enumerate(model.components_):
        topic_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
        topics.append(topic_words)
        print("Topic %d:" % (topic_idx), " ".join(topic_words))
    return topics

feature_names = vectorizer.get_feature_names_out()
no_top_words = 5
topics = display_topics(best_lda_model, feature_names, no_top_words)

texts_tokenized = [text.split() for text in combo]

# Create a Gensim dictionary from the tokenized data
dictionary = Dictionary(texts_tokenized)

# Convert tokenized documents to vectors
corpus = [dictionary.doc2bow(text) for text in texts_tokenized]

# Extract the top terms for each topic from the best LDA model
def get_lda_topics(model, num_topics):
    word_tokens = []
    for topic_id, topic in enumerate(model.components_):
        top_feature_idx = topic.argsort()[-10:][::-1]
        top_tokens = [feature_names[i] for i in top_feature_idx]
        word_tokens.append(top_tokens)
    return word_tokens



# Compute Coherence Score using Gensim's CoherenceModel
coherence_model_lda = CoherenceModel(topics=topics, texts=texts_tokenized, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()

print('\nCoherence Score: ', coherence_lda)

"""#tfidf best model"""

import re

def remove_non_alphanumeric(docs):
    cleaned_docs = []
    for doc in docs:
        cleaned_doc = ' '.join([word for word in doc.split() if word.isalnum()])
        cleaned_docs.append(cleaned_doc)
    return cleaned_docs

# Example usage:
documents = ["This is a test document with __proto__ and other words.", "Another document with __all__ and 1234."]
cleaned_documents = remove_non_alphanumeric(combo)
print(cleaned_documents)

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

def remove_stopwords(documents):
    """
    Remove stopwords from a list of documents.

    Args:
    - documents (list of str): List of document strings.

    Returns:
    - list of str: List of document strings with stopwords removed.
    """
    stop_words = set(stopwords.words('english'))
    return [' '.join(word for word in doc.split() if word.lower() not in stop_words) for doc in documents]

# Example usage:
cleaned_documents = remove_stopwords(cleaned_documents)

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer



# Vectorization: Convert documents to a matrix of token counts
vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')
tuning_sample_matrix = vectorizer.fit_transform(cleaned_documents)

# Setting up the LDA model with provided hyperparameters
tfidf_best_lda_model = LatentDirichletAllocation(
    n_components=28,               # Number of topics
    doc_topic_prior=0.31,          # Alpha
    topic_word_prior=0.31,         # Beta
    learning_method='online',      # Use 'online' or 'batch' learning
    random_state=0,                # Seed for reproducibility
    n_jobs=-1                      # Use all available CPUs
)

# Fitting the LDA model
tfidf_best_lda_model.fit(tuning_sample_matrix)



texts_tokenized = [text.split() for text in cleaned_documents]

# Create a Gensim dictionary from the tokenized data
dictionary = Dictionary(texts_tokenized)

# Convert tokenized documents to vectors
corpus = [dictionary.doc2bow(text) for text in texts_tokenized]

# Extract the top terms for each topic from the best LDA model
def get_lda_topics(model, num_topics):
    word_tokens = []
    for topic_id, topic in enumerate(model.components_):
        top_feature_idx = topic.argsort()[-10:][::-1]
        top_tokens = [feature_names[i] for i in top_feature_idx]
        word_tokens.append(top_tokens)
    return word_tokens

topics = display_topics(tfidf_best_lda_model, feature_names, no_top_words)

# Compute Coherence Score using Gensim's CoherenceModel
coherence_model_lda = CoherenceModel(topics=topics, texts=texts_tokenized, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()

print('\nCoherence Score: ', coherence_lda)

topics = display_topics(tfidf_best_lda_model, feature_names, no_top_words)

import gensim
from gensim.corpora import Dictionary
from gensim.models import LdaModel
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Assuming cleaned_documents is a list of preprocessed document strings

# Tokenize the documents
texts = [doc.split() for doc in cleaned_documents]

# Create a Gensim dictionary from the tokenized documents
id2word = Dictionary(texts)
# Filter out extreme values to keep only the most frequent 1000 words
id2word.filter_extremes(no_below=2, no_above=0.95, keep_n=1000)

# Convert tokenized documents into a document-term matrix (corpus)
corpus = [id2word.doc2bow(text) for text in texts]

# Create and train the LDA model
lda_model = LdaModel(
    corpus=corpus,
    id2word=id2word,
    num_topics=28,      # Number of topics
    alpha=0.31,         # Alpha value
    eta=0.31,           # Beta value (also called eta in Gensim)
    passes=10,          # Number of passes over the corpus
    random_state=0      # Seed for reproducibility
)

# Print topics


pyLDAvis.enable_notebook()

# Assuming lda_model is your trained Gensim LDA model
vis = gensimvis.prepare(lda_model, corpus, id2word)
vis

coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')
coherence = coherence_model.get_coherence()
print(f'Coherence Score: {coherence}')

for idx, topic in lda_model.print_topics(-1):
    # Split the topic string into word:probability pairs
    word_prob_pairs = topic.split(" + ")

    # Extract just the words from those pairs
    words = [pair.split("*")[1].strip('"') for pair in word_prob_pairs]

    # Join the words into a single string and print with topic number
    print(f"Topic {idx}: {' '.join(words)}\n")

yLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, corpus, id2word)
vis

